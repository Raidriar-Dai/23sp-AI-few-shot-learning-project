# Fundamental training parameters:
test_only: False
known_data_source: True
dataset_list: ['Dataset_BUSI_with_GT']
# dataset_list: ['10shot_cifar100_20200721', '10shot_country211_20210924', '10shot_food_101_20211007', '10shot_oxford_iiit_pets_20211007', '10shot_stanford_cars_20211007']
# dataset_list: ['10shot_cifar100_20200721']
# dataset_list: ['10shot_country211_20210924']
# dataset_list: ['10shot_food_101_20211007']
# dataset_list: ['10shot_oxford_iiit_pets_20211007']
# dataset_list: ['10shot_stanford_cars_20211007']
nb_classes: null
batch_size: 32
start_epoch: 0
epochs: 50
test_interval: 3  # for wandb test
bce_loss: False
unscale_lr: False

# Model Parameters:
model: swin_base_patch4_window12_384 # 7. 更改导入的模型
# model: gcvit_tiny
input_size: 384
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-8
clip_grad: null
momentum: 0.9
weight_decay: 0.01  # 6. configure weight_decay to mitigate overfitting.

# Learning rate schedule parameters
sched: cosine
lr: 5e-5
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-6
min_lr: 1e-6

decay_epochs: 0 # 对于 CosineLRScheduler 来说没用, 仅用于 StepLRScheduler
warmup_epochs: 10  # 只要 warmup_epochs=0, sched, warmup_lr 和 min_lr 就都不起作用.
cooldown_epochs: 0
patience_epochs: 0
decay_rate: 1 # 若使用 cosineLR 一般置为1, 使用 stepLR 等其他一般置为 0.1; 实际上 cosineLR 的实现中并没接收此参数.

# Augmentation parameters
rand_aug: True  # Set False if do not want any augmentation in training.
repeated_aug: True # 8. if repeated_aug == False, then the whole training set can be iterated over.
num_repeats: 5
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: 0
train_interpolation: bicubic
train_mode: True
src: False
flip: False  # 3. augment-1
rotation: False  # 4. augment-2

# * Finetuning params
# finetune: ''

# Dataset parameters
# data_path: /remote-home/share/course23/aicourse_dataset_final/
data_path: ./dataset/
data_set: IMNET
inat_category: name
output_dir: output/swin_base_patch4_window12_384/cosine # 1. needs to be customized each run.
# output_dir: output/gcvit_tiny/B_1
device: cuda
seed: 0
# resume: 'output/swin_tiny_patch4_window7_224/initial_config/best_checkpoint.pth' # 2. needs to be customized each run.
resume: ''
eval: False
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 0  # 5. to prevent dataloader from being killed.
pin_mem: True

# wandb parameters:
wandb:
  setup:
      project: sta221
      entity: raidriar_dai
      mode: online # may trigger network error
  watch:
      log: all
      log_freq: 100

# distributed training parameters
distributed: ''
world_size: 1
dist_url: env://
dist_backend: nccl
gpu: '' # to be assigned in utils.py

# Hydra parameters:
hydra:
  run:
    dir: logs